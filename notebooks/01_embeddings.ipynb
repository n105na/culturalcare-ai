{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c2bc51a-96d6-4b8d-abaa-959df423b1f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding model loaded. dim = 384\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import spacy\n",
    "from langdetect import detect\n",
    "import re\n",
    "# load models\n",
    "embed_model = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "nlp_en = spacy.load(\"en_core_web_sm\")\n",
    "nlp_fr = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "print(\"embedding model loaded. dim =\", embed_model.get_sentence_embedding_dimension())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6357e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Basic cleaning: lowercase, remove special chars, normalize spaces\"\"\"\n",
    "    text = text.strip().lower()\n",
    "    text = re.sub(r\"[^a-zA-ZÃ Ã¢Ã§Ã©Ã¨ÃªÃ«Ã®Ã¯Ã´Ã»Ã¹Ã¼Ã¿Ã±Ã¦Å“0-9\\u0600-\\u06FF\\s']\", \" \", text)  \n",
    "    # keep Latin, Arabic (0600-06FF), digits\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text\n",
    "\n",
    "def detect_language(text):\n",
    "    \"\"\"Detect language: English, French, Arabic, Darija\"\"\"\n",
    "    try:\n",
    "        lang = detect(text)\n",
    "        if lang in (\"en\", \"fr\", \"ar\"):\n",
    "            if lang == \"ar\":\n",
    "                # crude heuristic: if mix of Arabic + Latin chars â†’ Darija\n",
    "                if any(c in text for c in \"abcdefghijklmnopqrstuvwxyz\"):\n",
    "                    return \"darija\"\n",
    "                return \"ar\"\n",
    "            return lang\n",
    "    except:\n",
    "        pass\n",
    "    return \"unknown\"\n",
    "\n",
    "def tokenize_text(text, lang=\"en\"):\n",
    "    \"\"\"Tokenize text based on detected language\"\"\"\n",
    "    if lang == \"en\":\n",
    "        nlp = nlp_en\n",
    "    elif lang == \"fr\":\n",
    "        nlp = nlp_fr\n",
    "    else:\n",
    "        # fallback regex for Darija/Arabic\n",
    "        return re.findall(r\"\\w+\", text)\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    return [tok.text for tok in doc if not tok.is_punct and not tok.is_space]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e403d574-bf8d-4e4c-bdfc-0a22268357c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I feel very sad today and tired          -> en       -> ['I', 'feel', 'very', 'sad', 'today', 'and', 'tired']\n",
      "I am so happy and excited!               -> en       -> ['I', 'am', 'so', 'happy', 'and', 'excited']\n",
      "Je me sens triste et isolÃ©               -> fr       -> ['Je', 'me', 'sens', 'triste', 'et', 'isolÃ©']\n",
      "Je suis stressÃ© par mes examens          -> fr       -> ['Je', 'suis', 'stressÃ©', 'par', 'mes', 'examens']\n",
      "I am worried about tomorrow              -> en       -> ['I', 'am', 'worried', 'about', 'tomorrow']\n",
      "Je suis trÃ¨s content aujourd'hui         -> fr       -> ['Je', 'suis', 'trÃ¨s', 'content', \"aujourd'hui\"]\n",
      "Ø§Ù†Ø§ Ø­Ø²ÙŠÙ† Ø§Ù„ÙŠÙˆÙ…                           -> ar       -> ['Ø§Ù†Ø§', 'Ø­Ø²ÙŠÙ†', 'Ø§Ù„ÙŠÙˆÙ…']\n",
      "i suis stressed                          -> unknown  -> ['i', 'suis', 'stressed']\n",
      "ana dayekh bzaf had lyoum                -> unknown  -> ['ana', 'dayekh', 'bzaf', 'had', 'lyoum']\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    \"I feel very sad today and tired\",\n",
    "    \"I am so happy and excited!\",\n",
    "    \"Je me sens triste et isolÃ©\",\n",
    "    \"Je suis stressÃ© par mes examens\",\n",
    "    \"I am worried about tomorrow\",\n",
    "    \"Je suis trÃ¨s content aujourd'hui\",\n",
    "    \"Ø§Ù†Ø§ Ø­Ø²ÙŠÙ† Ø§Ù„ÙŠÙˆÙ…\",           # Arabic\n",
    "    \"je suis stressed \",           \n",
    "    \"ana dayekh bzaf had lyoum\" # Darija in Latin letters\n",
    "]\n",
    "\n",
    "for s in sentences:\n",
    "    lang = detect_language(s)\n",
    "    print(f\"{s:40} -> {lang:8} -> {tokenize_text(s, lang)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93921803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e98347dc28d49dfa0d82a37042f8699",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Embeddings shape: (9, 384)\n",
      "ðŸ”Ž Pairwise similarity matrix:\n",
      " [[1.   0.38 0.7  0.45 0.44 0.49 0.8  0.56 0.12]\n",
      " [0.38 1.   0.21 0.15 0.28 0.77 0.26 0.32 0.1 ]\n",
      " [0.7  0.21 1.   0.42 0.21 0.19 0.66 0.46 0.03]\n",
      " [0.45 0.15 0.42 1.   0.3  0.17 0.44 0.77 0.12]\n",
      " [0.44 0.28 0.21 0.3  1.   0.27 0.47 0.34 0.01]\n",
      " [0.49 0.77 0.19 0.17 0.27 1.   0.42 0.31 0.08]\n",
      " [0.8  0.26 0.66 0.44 0.47 0.42 1.   0.48 0.08]\n",
      " [0.56 0.32 0.46 0.77 0.34 0.31 0.48 1.   0.18]\n",
      " [0.12 0.1  0.03 0.12 0.01 0.08 0.08 0.18 1.  ]]\n"
     ]
    }
   ],
   "source": [
    "cleaned = [clean_text(s) for s in sentences]\n",
    "embeddings = embed_model.encode(cleaned, show_progress_bar=True)\n",
    "print(\"âœ… Embeddings shape:\", embeddings.shape)\n",
    "\n",
    "# pairwise similarity\n",
    "sim = cosine_similarity(embeddings)\n",
    "np.set_printoptions(precision=2, suppress=True)\n",
    "print(\"ðŸ”Ž Pairwise similarity matrix:\\n\", sim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c068abe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm feeling hopeless and sad today  -> [('anxious', 0.448197603225708)]\n",
      "Je suis super content!              -> [('happy', 0.7933486104011536)]\n",
      "s7i7 mlih bzaf                      -> [('sad', 0.20121295750141144)]\n",
      "Ø§Ù†Ø§ Ù…ØªÙˆØªØ± Ø¨Ø²Ø§Ù                      -> [('anxious', 0.6432017087936401)]\n"
     ]
    }
   ],
   "source": [
    "def predict_nearest_label(input_text, candidates, k=1):\n",
    "    \"\"\"Find nearest label using cosine similarity\"\"\"\n",
    "    emb = embed_model.encode([clean_text(input_text)])\n",
    "    cand_emb = embed_model.encode([clean_text(c) for c in candidates])\n",
    "    sims = cosine_similarity(emb, cand_emb)[0]\n",
    "    best_idx = sims.argsort()[::-1][:k]\n",
    "    return [(candidates[i], float(sims[i])) for i in best_idx]\n",
    "\n",
    "candidates = [\"sad\", \"happy\", \"anxious\", \"neutral\", \"stressed\"]\n",
    "\n",
    "tests = [\n",
    "    \"I'm feeling hopeless and sad today\",\n",
    "    \"Je suis super content!\",\n",
    "    \"s7i7 mlih bzaf\",\n",
    "    \"Ø§Ù†Ø§ Ù…ØªÙˆØªØ± Ø¨Ø²Ø§Ù\"\n",
    "]\n",
    "\n",
    "for t in tests:\n",
    "    print(f\"{t:35} -> {predict_nearest_label(t, candidates)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bcde1d-f671-49ec-8f88-9376dbf7b8a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
